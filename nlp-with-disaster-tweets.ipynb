{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with Disaster Tweets\n",
    "\n",
    "This notebook contains code for training BERT using Pytorch and Hugging Face on the dataset used in the Kaggle Competition Natural Language Processing with Disaster Tweets (https://www.kaggle.com/competitions/nlp-getting-started/)\n",
    "\n",
    "# Dataset\n",
    "\n",
    "## Files\n",
    "\n",
    "train.csv - the training set\n",
    "\n",
    "test.csv - the test set\n",
    "\n",
    "sample_submission.csv - a sample submission file to submit the predictions on Kaggle\n",
    "\n",
    "## Columns\n",
    "\n",
    "id - id for each tweet\n",
    "\n",
    "text - the text of the tweet\n",
    "\n",
    "location - the location the tweet was sent from\n",
    "\n",
    "keyword - a particular keyword from the tweet\n",
    "\n",
    "target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n",
    "\n",
    "### The goal is to predict if the tweet is about a disaster or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-02T23:00:10.073667Z",
     "iopub.status.busy": "2022-04-02T23:00:10.073315Z",
     "iopub.status.idle": "2022-04-02T23:00:12.483823Z",
     "shell.execute_reply": "2022-04-02T23:00:12.483119Z",
     "shell.execute_reply.started": "2022-04-02T23:00:10.073576Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-02T19:14:05.662611Z",
     "iopub.status.busy": "2022-04-02T19:14:05.662358Z",
     "iopub.status.idle": "2022-04-02T19:14:05.667417Z",
     "shell.execute_reply": "2022-04-02T19:14:05.666170Z",
     "shell.execute_reply.started": "2022-04-02T19:14:05.662582Z"
    }
   },
   "outputs": [],
   "source": [
    "#Model hyperparameter configs\n",
    "class CONFIG:\n",
    "    MODEL_PATH = './model/'\n",
    "    SAVE_EVERT = 10\n",
    "    EPOCHS = 10\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 1e-5\n",
    "    TRAIN_TEST_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Training Procedure\n",
    "\n",
    "Task 1: Masked Language Modelling- 15% of the tokens are masked out and the model needs to predict the masked out tokens.\n",
    "\n",
    "<img src=\"images/11.png\">\n",
    "\n",
    "Task 2: Two-sentence Tasks- Given two setences, sentence A and sentence B, the model needs to predict if sentence B follows sentence A or not. This is a binary prediction task.\n",
    "\n",
    "<img src=\"images/12.png\">\n",
    "\n",
    "The BERT model is pre-trained using these two tasks.\n",
    "\n",
    "https://jalammar.github.io/illustrated-bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-02T19:14:43.336669Z",
     "iopub.status.busy": "2022-04-02T19:14:43.336093Z",
     "iopub.status.idle": "2022-04-02T19:14:43.343914Z",
     "shell.execute_reply": "2022-04-02T19:14:43.343229Z",
     "shell.execute_reply.started": "2022-04-02T19:14:43.336628Z"
    }
   },
   "outputs": [],
   "source": [
    "# BERT CLass\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_labels, dropout=0.1, freeze_bert=True):\n",
    "        super(BERT, self).__init__()\n",
    "        # Import BERT from HuggingFace Transformer library\n",
    "        self.bert = transformers.BertModel.from_pretrained(bert_model_name)\n",
    "        # Freeze weights of Pretrained BERT Model. These weights will not be\n",
    "        # updated during training.\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        # Add Dropout Layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Add a Classifier Layer for finetune the model and get the output=number of labels.\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        # Forward Function to pass the data.\n",
    "        # Pass the data thriugh BERT\n",
    "        _, pooled_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        # Add Dropout Layer to the BERT Output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        # Get Classifier Output Logits\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-02T19:14:43.902699Z",
     "iopub.status.busy": "2022-04-02T19:14:43.901979Z",
     "iopub.status.idle": "2022-04-02T19:14:43.913451Z",
     "shell.execute_reply": "2022-04-02T19:14:43.912762Z",
     "shell.execute_reply.started": "2022-04-02T19:14:43.902660Z"
    }
   },
   "outputs": [],
   "source": [
    "class DisasterTweetsDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512, train=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.df = df\n",
    "        self.train = train\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get Text Data\n",
    "        text = str(self.df.iloc[idx][\"text\"])\n",
    "        if self.train:\n",
    "            # Get labels\n",
    "            targets = torch.tensor(self.df.iloc[idx][\"target\"], dtype=torch.long)\n",
    "        \n",
    "        # Use the BERT Tokenizer on the Inputs.\n",
    "        # Add special tokens for start and end of text, pad the input texts to get them in equal length,\n",
    "        # Get attention mask on the padded inputs, get token_type_ids and return the outputs in the form \n",
    "        # of Pytorch Tensor.\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids, attention_mask, token_type_ids = inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"token_type_ids\"]\n",
    "        # In case of training, there is an additional targets output.\n",
    "        if self.train:\n",
    "            return {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "                \"targets\": targets,\n",
    "                }\n",
    "        else:\n",
    "            return {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"token_type_ids\": token_type_ids,\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-02T19:14:45.352394Z",
     "iopub.status.busy": "2022-04-02T19:14:45.352146Z",
     "iopub.status.idle": "2022-04-02T19:14:45.488335Z",
     "shell.execute_reply": "2022-04-02T19:14:45.487617Z",
     "shell.execute_reply.started": "2022-04-02T19:14:45.352366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"loading data\")\n",
    "# Get Train and Validation Data. Get the Train-Val split from the CONFIG.\n",
    "df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "train_df, valid_df = train_test_split(df, test_size=CONFIG.TRAIN_TEST_SPLIT, random_state=42)\n",
    "test_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n",
    "# Get the sample submission to submit the prediction to Kaggle.\n",
    "sub = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-02T19:14:48.004751Z",
     "iopub.status.busy": "2022-04-02T19:14:48.004170Z",
     "iopub.status.idle": "2022-04-02T19:15:01.673569Z",
     "shell.execute_reply": "2022-04-02T19:15:01.672844Z",
     "shell.execute_reply.started": "2022-04-02T19:14:48.004707Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the bert model to the device. Number of labels is 1 because there is one output field that can be \n",
    "# 0 or 1 i.e. it is a disaster (1) or not (0).\n",
    "model = BERT(\"../input/huggingface-bert/bert-base-cased\", num_labels=1, dropout=0.1).to(device)\n",
    "# Get tokenizer output.\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"../input/huggingface-bert/bert-base-cased\")\n",
    "# Get train, valid and test datasets\n",
    "train_dataset = DisasterTweetsDataset(train_df, tokenizer, max_len=512, train=True)\n",
    "valid_dataset = DisasterTweetsDataset(valid_df, tokenizer, max_len=512, train=True)\n",
    "test_dataset = DisasterTweetsDataset(test_df, tokenizer, max_len=512, train=False)\n",
    "# Use AdamW Optimizer\n",
    "optimizer = transformers.AdamW(model.parameters(), lr=2e-5)\n",
    "# Use Learning rate scheduler\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(train_dataset) * 5,\n",
    ")\n",
    "# Loss Function\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-02T19:15:12.182861Z",
     "iopub.status.busy": "2022-04-02T19:15:12.182425Z",
     "iopub.status.idle": "2022-04-02T19:15:12.197896Z",
     "shell.execute_reply": "2022-04-02T19:15:12.196387Z",
     "shell.execute_reply.started": "2022-04-02T19:15:12.182816Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, train_dataloader, device):\n",
    "    # Call the model training function.\n",
    "    model.train()\n",
    "    # Training Loss\n",
    "    total_loss = 0\n",
    "    bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "    for i, data in bar:\n",
    "        # Zero grad for optimizer before starting the training batch.\n",
    "        optimizer.zero_grad()\n",
    "        # Pass the input_ids, attention_mask, token_type_ids to the model.\n",
    "        input_ids = data[\"input_ids\"].to(device).squeeze(1)\n",
    "        attention_mask = data[\"attention_mask\"].to(device).squeeze(1)\n",
    "        token_type_ids = data[\"token_type_ids\"].to(device).squeeze(1)\n",
    "        targets = data[\"targets\"].to(device).unsqueeze(1)\n",
    "        out = model(input_ids, attention_mask, token_type_ids)\n",
    "        # Get the model loss.\n",
    "        loss = criterion(out, targets.float())\n",
    "        bar.set_postfix({\n",
    "                \"Train Loss\": \"{:.6f}\".format(abs(loss)),}\n",
    "        )\n",
    "        # Backpropagate the loss.\n",
    "        loss.backward()\n",
    "        # Update the model parameters.\n",
    "        optimizer.step()\n",
    "        # Update the learning rate scheduler.\n",
    "        scheduler.step()\n",
    "        # Add the batch loss to the epoch loss.\n",
    "        total_loss += loss.item()\n",
    "    # Normalize the loss.\n",
    "    return total_loss / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-02T19:15:22.016277Z",
     "iopub.status.busy": "2022-04-02T19:15:22.015828Z",
     "iopub.status.idle": "2022-04-02T19:15:22.023138Z",
     "shell.execute_reply": "2022-04-02T19:15:22.022453Z",
     "shell.execute_reply.started": "2022-04-02T19:15:22.016239Z"
    }
   },
   "outputs": [],
   "source": [
    "def valid_one_epoch(model, valid_dataloader, device):\n",
    "    # Change the model to eval mode.\n",
    "    model.eval()\n",
    "    # Validation Loss\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        bar = tqdm(enumerate(valid_dataloader), total=len(valid_dataloader))\n",
    "        for i, data in bar:\n",
    "            # Pass the input_ids, attention_mask, token_type_ids to the model.\n",
    "            input_ids = data[\"input_ids\"].to(device).squeeze(1)\n",
    "            attention_mask = data[\"attention_mask\"].to(device).squeeze(1)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(device).squeeze(1)\n",
    "            targets = data[\"targets\"].to(device).unsqueeze(1)\n",
    "            out = model(input_ids, attention_mask, token_type_ids)\n",
    "            # Get the model loss.\n",
    "            loss = criterion(out, targets.float())\n",
    "            bar.set_postfix({\n",
    "                \"Valid Loss\": \"{:.6f}\".format(abs(loss)),}\n",
    "            )\n",
    "            # Add the batch loss to the epoch loss.\n",
    "            total_loss += loss.item()\n",
    "    # Normalize the loss.\n",
    "    return total_loss / len(valid_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-02T19:15:24.739312Z",
     "iopub.status.busy": "2022-04-02T19:15:24.738837Z",
     "iopub.status.idle": "2022-04-02T19:15:24.743479Z",
     "shell.execute_reply": "2022-04-02T19:15:24.742842Z",
     "shell.execute_reply.started": "2022-04-02T19:15:24.739274Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataloader(train_dataset, valid_dataset, batch_size=CONFIG.BATCH_SIZE):\n",
    "    # Get the Dataloader for Train and Validation datasets.\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_dataloader, valid_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-02T19:15:28.737453Z",
     "iopub.status.busy": "2022-04-02T19:15:28.736968Z",
     "iopub.status.idle": "2022-04-02T19:15:28.745926Z",
     "shell.execute_reply": "2022-04-02T19:15:28.743330Z",
     "shell.execute_reply.started": "2022-04-02T19:15:28.737411Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store train and valid losses after each epoch.\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "def train():\n",
    "    # Get the train and valid dataloader\n",
    "    train_dataloader, valid_dataloader = get_dataloader(train_dataset, valid_dataset)\n",
    "    # Update the best valid loss each\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    for epoch in range(1, CONFIG.EPOCHS + 1):\n",
    "        # Get the train loss for the epoch.\n",
    "        train_loss = train_one_epoch(model, optimizer, scheduler, train_dataloader, device)\n",
    "        # Get the validation loss for the epoch.\n",
    "        valid_loss = valid_one_epoch(model, valid_dataloader, device)\n",
    "        # Store the train and valid loss in the list.\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        print(f\"Epoch {epoch}: Training Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}\")\n",
    "        # Update the best valid loss each time a model gets better performance.\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"best_model.bin\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-02T19:29:58.071141Z",
     "iopub.status.busy": "2022-04-02T19:29:58.070401Z",
     "iopub.status.idle": "2022-04-02T19:55:44.323679Z",
     "shell.execute_reply": "2022-04-02T19:55:44.322827Z",
     "shell.execute_reply.started": "2022-04-02T19:29:58.071097Z"
    }
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loss = 0.6576\n",
    "### Validation Loss= 0.6454"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
